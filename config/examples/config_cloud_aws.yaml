# ============================================================================
# RAPTOR v2.1.0 - AWS CLOUD DEPLOYMENT CONFIGURATION
# ============================================================================
# 
# This configuration is optimized for running RAPTOR on AWS cloud infrastructure.
# Perfect for large-scale analyses, batch processing, and institutional deployments.
#
# Features:
# - AWS Batch integration for parallel processing
# - S3 storage for data and results
# - EC2 spot instances for cost savings
# - Auto-scaling based on workload
# - CloudWatch monitoring
#
# Prerequisites:
# - AWS account with appropriate permissions
# - AWS CLI configured with credentials
# - S3 bucket created for RAPTOR data
# - AWS Batch compute environment set up
#
# Usage: raptor profile --counts s3://bucket/data.csv --config config_cloud_aws.yaml
# ============================================================================

# ============================================================================
# GENERAL SETTINGS
# ============================================================================
general:
  version: "2.1.0"
  verbose: true
  log_level: "INFO"
  output_dir: "s3://your-raptor-bucket/outputs"  # S3 path
  temp_dir: "/tmp/raptor"  # Local temp on EC2 instance

# ============================================================================
# COMPUTATIONAL RESOURCES - CLOUD OPTIMIZED
# ============================================================================
resources:
  # These will be overridden by EC2 instance specs
  default_threads: 16
  default_memory_gb: 64
  max_threads: 32
  max_memory_gb: 128
  
  # Resource allocation per analysis type
  profiling:
    threads: 8
    memory_gb: 16
  
  benchmarking:
    threads: 16
    memory_gb: 64
  
  ml_training:
    threads: 16
    memory_gb: 32
  
  ensemble_analysis:
    threads: 24
    memory_gb: 96
  
  # GPU settings (for GPU instances like p3.2xlarge)
  gpu:
    enabled: false  # Set true if using GPU instances
    device_id: 0
    memory_fraction: 0.9

# ============================================================================
# AWS CLOUD INTEGRATION
# ============================================================================
cloud:
  enabled: true
  provider: "aws"
  
  # ========================================================================
  # AWS CONFIGURATION
  # ========================================================================
  aws:
    # Region selection - choose closest to your data
    region: "us-east-1"  # Change to your preferred region
    
    # S3 storage
    bucket: "your-raptor-bucket"  # CHANGE THIS to your bucket name
    prefix: "raptor-analyses"  # Folder structure in S3
    
    # EC2 instance configuration
    instance_type: "c5.4xlarge"  # 16 vCPUs, 32 GB RAM
    # Other good options:
    #   - c5.2xlarge: 8 vCPUs, 16 GB (smaller jobs)
    #   - c5.9xlarge: 36 vCPUs, 72 GB (large jobs)
    #   - r5.4xlarge: 16 vCPUs, 128 GB (memory-intensive)
    #   - m5.8xlarge: 32 vCPUs, 128 GB (balanced)
    
    # Spot instances for cost savings (60-90% cheaper)
    spot_instances: true
    max_spot_price: 0.50  # USD per hour
    spot_interruption_behavior: "terminate"  # terminate, stop, hibernate
    
    # AMI (Amazon Machine Image)
    ami_id: null  # Use default Amazon Linux 2023 if null
    custom_ami: false
    
    # Security
    security_group_ids: []  # Add your security group IDs
    subnet_id: null  # Specify if using VPC
    iam_role: "RAPTORExecutionRole"  # IAM role for EC2/Batch
    
    # SSH access (optional)
    key_pair_name: null  # SSH key for debugging
    
    # ========================================================================
    # AWS BATCH CONFIGURATION
    # ========================================================================
    batch:
      enabled: true
      
      # Job queue and definition
      job_queue: "raptor-job-queue"
      job_definition: "raptor-job-def"
      
      # Compute resources
      vcpus: 16
      memory: 32768  # MB
      
      # Retry strategy
      retry_attempts: 2
      retry_evaluation: "Exit code is not 0"
      
      # Timeout
      timeout_seconds: 86400  # 24 hours
      
      # Job dependencies
      enable_dependencies: true
      
      # Array jobs for parallel processing
      array_jobs:
        enabled: true
        size: 10  # Number of parallel jobs
    
    # ========================================================================
    # S3 DATA MANAGEMENT
    # ========================================================================
    s3:
      # Input data location
      input_bucket: "your-raptor-bucket"
      input_prefix: "input-data"
      
      # Output data location
      output_bucket: "your-raptor-bucket"
      output_prefix: "results"
      
      # Storage class
      storage_class: "STANDARD"  # STANDARD, INTELLIGENT_TIERING, GLACIER
      
      # Lifecycle policies
      lifecycle:
        enabled: true
        archive_after_days: 90  # Move to Glacier after 90 days
        delete_after_days: 365  # Delete after 1 year
      
      # Versioning
      versioning: true
      
      # Encryption
      encryption:
        enabled: true
        kms_key_id: null  # Use default S3 encryption if null
      
      # Transfer acceleration
      transfer_acceleration: false
      
      # Multipart upload settings
      multipart_threshold: 8388608  # 8 MB
      multipart_chunksize: 8388608
      max_concurrency: 10
    
    # ========================================================================
    # CLOUDWATCH MONITORING
    # ========================================================================
    cloudwatch:
      enabled: true
      
      # Log groups
      log_group: "/aws/batch/raptor"
      log_stream_prefix: "raptor-job"
      
      # Metrics to track
      custom_metrics:
        - "JobDuration"
        - "DataProcessed"
        - "PipelineSuccess"
        - "ErrorRate"
      
      # Alarms
      alarms:
        enabled: true
        
        # Job failure alarm
        job_failure:
          threshold: 3  # Alert after 3 failures
          period: 300  # 5 minutes
          evaluation_periods: 1
          sns_topic: "arn:aws:sns:us-east-1:ACCOUNT:raptor-alerts"
        
        # Cost alarm
        cost_alarm:
          threshold: 100  # USD per day
          sns_topic: "arn:aws:sns:us-east-1:ACCOUNT:billing-alerts"
    
    # ========================================================================
    # AUTO-SCALING
    # ========================================================================
    autoscaling:
      enabled: true
      
      # Compute environment scaling
      min_vcpus: 0  # Scale to 0 when idle
      desired_vcpus: 16
      max_vcpus: 256  # Maximum parallel capacity
      
      # Scaling policies
      scale_up:
        metric: "queue_depth"
        threshold: 10  # Jobs waiting
        adjustment: 32  # Add 32 vCPUs
      
      scale_down:
        metric: "idle_time"
        threshold: 300  # 5 minutes idle
        adjustment: -16  # Remove 16 vCPUs
    
    # ========================================================================
    # NETWORKING
    # ========================================================================
    networking:
      # VPC configuration
      vpc_id: null
      subnets: []  # List of subnet IDs
      
      # Internet access
      assign_public_ip: true
      
      # Bandwidth optimization
      enhanced_networking: true
      
      # Data transfer
      use_vpc_endpoint: true  # For S3 access without internet
    
    # ========================================================================
    # COST OPTIMIZATION
    # ========================================================================
    cost_optimization:
      # Spot instance strategy
      spot_fleet:
        enabled: true
        allocation_strategy: "lowestPrice"  # lowestPrice, diversified
        instance_types:
          - "c5.4xlarge"
          - "c5a.4xlarge"
          - "c5n.4xlarge"
          - "m5.4xlarge"
      
      # Reserved instances
      use_reserved_instances: false
      
      # Savings plans
      use_savings_plans: false
      
      # Budget alerts
      budget:
        enabled: true
        monthly_limit: 500  # USD
        alert_threshold: 80  # Alert at 80% of budget

# ============================================================================
# DATA PROFILING
# ============================================================================
profiling:
  bcv_thresholds:
    very_low: 0.2
    low: 0.4
    moderate: 0.6
    high: 0.8
  
  depth_categories:
    low: 10
    moderate: 30
    high: 50
  
  sample_size:
    small: 6
    medium: 12
    large: 20
  
  # Cloud-optimized settings
  min_counts_per_gene: 10
  min_samples_per_gene: 3
  filter_low_counts: true
  
  # Save to S3
  generate_plots: true
  save_profile: true
  profile_format: "json"

# ============================================================================
# ML RECOMMENDATION - CLOUD OPTIMIZED
# ============================================================================
ml_recommendation:
  enabled: true
  model_type: "random_forest"
  
  # Load model from S3
  model_path: "s3://your-raptor-bucket/models/raptor_rf_model.pkl"
  scaler_path: "s3://your-raptor-bucket/models/raptor_scaler.pkl"
  
  prediction:
    confidence_threshold: 0.7
    top_n_recommendations: 3
    explain_predictions: true
  
  # Save predictions to S3
  performance:
    track_metrics: true
    save_metrics: true
    metrics_file: "s3://your-raptor-bucket/metrics/ml_performance.json"

# ============================================================================
# QUALITY ASSESSMENT
# ============================================================================
quality_assessment:
  enabled: true
  
  metrics:
    - "library_size"
    - "detected_genes"
    - "mitochondrial_content"
    - "mapping_rate"
  
  thresholds:
    min_library_size: 1e6
    min_detected_genes: 5000
    max_mitochondrial_pct: 10
    min_mapping_rate: 70
  
  generate_qc_report: true
  qc_report_format: "html"

# ============================================================================
# RESOURCE MONITORING - CLOUDWATCH INTEGRATION
# ============================================================================
resource_monitoring:
  enabled: true
  sampling_interval: 30.0  # Less frequent for cloud (reduce logging costs)
  
  track_metrics:
    - "cpu_percent"
    - "memory_percent"
    - "disk_io"
    - "network_io"
  
  # Alert to CloudWatch
  alerts:
    enabled: true
    cpu_threshold: 90
    memory_threshold: 85
    alert_method: "cloudwatch"
  
  # Log to CloudWatch
  save_metrics: true
  metrics_file: "cloudwatch"  # Special value for CloudWatch Logs

# ============================================================================
# ENSEMBLE ANALYSIS - PARALLEL CLOUD EXECUTION
# ============================================================================
ensemble:
  enabled: false
  
  methods:
    - "rank_aggregation"
    - "vote_counting"
    - "weighted_average"
  
  # Cloud parallel execution
  parallel_execution: true
  max_parallel_pipelines: 5
  
  generate_ensemble_report: true

# ============================================================================
# CONTAINER CONFIGURATION
# ============================================================================
cloud:
  container:
    enabled: true
    
    # Docker image
    image: "ayehblk/raptor:2.1.0"
    registry: "docker.io"
    
    # ECR (Elastic Container Registry) alternative
    use_ecr: false
    ecr_repository: "your-account.dkr.ecr.us-east-1.amazonaws.com/raptor"
    
    # Container resources
    vcpus: 16
    memory: 32768
    
    # Environment variables
    environment:
      RAPTOR_ENV: "production"
      AWS_DEFAULT_REGION: "us-east-1"
      RAPTOR_LOG_LEVEL: "INFO"
    
    # Volumes (for EFS if needed)
    volumes: []
    
    pull_policy: "IfNotPresent"

# ============================================================================
# PARALLEL PROCESSING
# ============================================================================
cloud:
  parallel:
    enabled: true
    max_workers: 20  # AWS Batch will handle this
    chunk_size: 1000
    strategy: "data_parallel"
    
    # Distributed computing
    use_dask: false  # Optional: Dask for distributed computing
    dask_scheduler: null

# ============================================================================
# DATA TRANSFER OPTIMIZATION
# ============================================================================
cloud:
  data_transfer:
    method: "s3"
    
    # Compression to reduce transfer costs
    compression: true
    compression_format: "gzip"
    
    # Encryption in transit
    encryption: true
    
    # Prefetch data to local disk
    prefetch: true
    prefetch_size_gb: 10
    
    # Upload results incrementally
    incremental_upload: true
    upload_interval: 300  # 5 minutes

# ============================================================================
# BENCHMARKING - CLOUD SCALE
# ============================================================================
benchmarking:
  mode: "comprehensive"
  
  # Use S3 for benchmark data
  use_simulated_data: false
  benchmark_data_path: "s3://your-raptor-bucket/benchmarks"
  
  save_benchmarks: true
  benchmark_dir: "s3://your-raptor-bucket/benchmark-results"

# ============================================================================
# OUTPUT SETTINGS - S3 OPTIMIZED
# ============================================================================
output:
  formats:
    results: ["csv", "tsv"]
    plots: ["png", "pdf"]
    reports: ["html", "pdf"]
  
  # Compress to reduce S3 storage costs
  compress_outputs: true
  compression_format: "gzip"
  
  # Upload to S3
  keep_intermediate_files: false
  cleanup_temp: true
  
  # S3 specific
  s3_storage_class: "INTELLIGENT_TIERING"  # Automatic cost optimization

# ============================================================================
# AUTOMATED REPORTING
# ============================================================================
automated_reporting:
  enabled: true
  formats: ["html", "pdf"]
  
  sections:
    - "executive_summary"
    - "data_profiling"
    - "quality_assessment"
    - "pipeline_recommendation"
    - "differential_expression"
    - "resource_usage"
    - "cloud_metrics"
  
  # Save reports to S3
  output_location: "s3://your-raptor-bucket/reports"
  
  interpretation:
    enabled: true
    databases: ["GO", "KEGG"]

# ============================================================================
# NOTIFICATIONS - AWS SNS
# ============================================================================
notifications:
  enabled: true
  
  # Email via SNS
  email:
    enabled: true
    sns_topic_arn: "arn:aws:sns:us-east-1:ACCOUNT:raptor-notifications"
    notify_on: ["completion", "error", "warning"]
  
  # Slack via SNS â†’ Lambda
  slack:
    enabled: false
    sns_topic_arn: "arn:aws:sns:us-east-1:ACCOUNT:raptor-slack"
    channel: "#bioinformatics"

# ============================================================================
# SECURITY & COMPLIANCE
# ============================================================================
security:
  # Encryption
  encrypt_at_rest: true
  encrypt_in_transit: true
  
  # IAM
  use_iam_roles: true
  least_privilege: true
  
  # Audit logging
  enable_cloudtrail: true
  
  # Data privacy
  phi_mode: false  # Set true for HIPAA compliance
  
  # Network security
  restrict_outbound: false
  allowed_egress:
    - "0.0.0.0/0"  # Change for production

# ============================================================================
# COST TRACKING
# ============================================================================
cost_tracking:
  enabled: true
  
  # Tag resources for cost allocation
  tags:
    Project: "RAPTOR"
    Environment: "Production"
    CostCenter: "Bioinformatics"
    PI: "Dr. Smith"
  
  # Cost reporting
  generate_cost_report: true
  report_interval: "daily"  # daily, weekly, monthly
  
  # Budget alerts
  alert_on_cost_overrun: true
  cost_threshold: 50  # USD per day

# ============================================================================
# DEPLOYMENT NOTES
# ============================================================================
# 
# SETUP INSTRUCTIONS:
# 
# 1. Create S3 bucket:
#    aws s3 mb s3://your-raptor-bucket --region us-east-1
# 
# 2. Create IAM role with policies:
#    - AmazonS3FullAccess
#    - AWSBatchServiceRole
#    - CloudWatchLogsFullAccess
# 
# 3. Create AWS Batch compute environment:
#    - Name: raptor-compute-env
#    - Instance types: c5.4xlarge, c5a.4xlarge
#    - Min vCPUs: 0, Max vCPUs: 256
#    - Enable spot instances
# 
# 4. Create job queue:
#    aws batch create-job-queue \
#      --job-queue-name raptor-job-queue \
#      --compute-environment-order order=1,computeEnvironment=raptor-compute-env
# 
# 5. Build and push Docker image:
#    docker build -t raptor:2.1.0 .
#    docker tag raptor:2.1.0 ayehblk/raptor:2.1.0
#    docker push ayehblk/raptor:2.1.0
# 
# 6. Upload models and data to S3:
#    aws s3 sync ./models s3://your-raptor-bucket/models
#    aws s3 sync ./data s3://your-raptor-bucket/input-data
# 
# 7. Run RAPTOR job:
#    raptor profile \
#      --counts s3://your-raptor-bucket/input-data/counts.csv \
#      --config config_cloud_aws.yaml \
#      --cloud
# 
# COST ESTIMATION:
# - c5.4xlarge spot: ~$0.20-0.40/hour
# - S3 storage: ~$0.023/GB/month
# - Data transfer: $0.09/GB out
# - Typical RNA-seq analysis: $2-10 per sample
# 
# TROUBLESHOOTING:
# - Check CloudWatch Logs: /aws/batch/raptor
# - Monitor Batch jobs: AWS Batch Console
# - S3 access issues: Check IAM role permissions
# - Spot instance interruptions: Enable retry strategy
# 
# ============================================================================
