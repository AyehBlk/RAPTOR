# RAPTOR v2.1.1 Profile & Recommend Guide

Get intelligent ML-powered pipeline recommendations and **data-driven thresholds** in seconds!

## Quick Start

```bash
# Basic profiling with ML
raptor profile --counts your_data.csv --use-ml

# With metadata and quality check
raptor profile \
  --counts data.csv \
  --metadata samples.csv \
  --use-ml \
  --quality-check

# Save interactive HTML report
raptor profile \
  --counts data.csv \
  --use-ml \
  --output report.html
```

## üÜï What's New in v2.1.1

### üéØ Adaptive Threshold Optimizer (ATO)

After getting your pipeline recommendation, use ATO for data-driven thresholds:

```python
from raptor.threshold_optimizer import optimize_thresholds

# After running your recommended pipeline
result = optimize_thresholds(de_results, goal='balanced')
print(f"Optimal |logFC| > {result.logfc_threshold:.2f}")
print(result.methods_text)  # Copy to paper!
```

---

## All Features

‚≠ê **ML-Powered Recommendations** - Learn from 10,000+ past analyses  
‚≠ê **Confidence Scores** - Know how reliable recommendations are  
‚≠ê **Similar Projects** - See what worked for similar data  
‚≠ê **Quality Assessment** - Automatic data QC  
‚≠ê **Feature Importance** - Understand WHY each pipeline was recommended  
‚≠ê **Resource Prediction** - Estimate CPU/memory/time needs  
‚≠ê **üéØ Threshold Optimization** - Data-driven thresholds (NEW v2.1.1!)

---

## How It Works

### Complete v2.1.1 Workflow

```
1. Analyzes your data ‚Üí BCV, depth, zeros, library sizes
2. Runs quality checks ‚Üí Contamination, outliers, batch effects
3. ML model prediction ‚Üí Based on 10,000+ similar analyses
4. Confidence scoring ‚Üí How sure is the model?
5. Find similar projects ‚Üí What worked before?
6. Recommends top 3 ‚Üí With ML scores, confidence, and reasoning
7. üéØ Run pipeline ‚Üí Get DE results
8. üéØ Optimize thresholds ‚Üí Data-driven cutoffs (NEW!)
9. üéØ Get methods text ‚Üí Publication-ready (NEW!)
```

---

## Understanding Output

### ML-Enhanced Output

```
#1 RECOMMENDED: Pipeline 3 (Salmon-edgeR)
   Rule-based Score: 0.88 | ML Score: 0.92 ‚≠ê
   Confidence: HIGH (89%)
   
   Why this pipeline?
   ‚úì Excellent balance for your data
   ‚úì Handles medium BCV well (yours: 0.42)
   ‚úì Fast runtime (~22 min estimated)
   ‚úì Low memory (12GB peak predicted)
   
   ML Insights:
   ‚Üí Based on 1,247 similar analyses
   ‚Üí 87.3% historical success rate on similar data
   ‚Üí Top features: BCV (35%), Sample size (22%), Depth (18%)
   
   Similar Successful Projects:
   ‚Ä¢ Project #8472: 12 samples, BCV=0.41, Human ‚Üí Success ‚úì
   ‚Ä¢ Project #2391: 10 samples, BCV=0.38, Human ‚Üí Success ‚úì
   
   Resource Predictions:
   ‚Ä¢ Runtime: 22 minutes (¬±5 min)
   ‚Ä¢ Peak Memory: 12 GB (¬±2 GB)

   üéØ Next Step (NEW v2.1.1):
   After running this pipeline, use ATO for optimal thresholds:
   ‚Üí raptor optimize-thresholds --input results.csv --goal balanced
```

---

## üéØ Complete Workflow with ATO (NEW)

### Step 1: Profile & Get Recommendation

```bash
raptor profile --counts data.csv --use-ml
# Output: Recommended Pipeline 3 (Salmon-edgeR)
```

### Step 2: Run Recommended Pipeline

```bash
raptor run --pipeline 3 --data fastq/ --output results/
```

### Step 3: Optimize Thresholds (NEW!)

```python
from raptor.threshold_optimizer import optimize_thresholds
import pandas as pd

# Load DE results
df = pd.read_csv('results/edger_results.csv')

# Optimize thresholds
result = optimize_thresholds(
    df,
    logfc_col='logFC',      # edgeR column name
    pvalue_col='PValue',    # edgeR column name
    goal='balanced'
)

# View results
print(f"Optimal |logFC| threshold: {result.logfc_threshold:.3f}")
print(f"Optimal p-value threshold: {result.pvalue_threshold}")
print(f"Significant genes: {result.n_significant}")

# Get publication methods text
print("\nüìù Methods text for your paper:")
print(result.methods_text)

# Save optimized results
result.results_df.to_csv('results/optimized_results.csv')
```

### Step 4: Generate Report

```bash
raptor report --results results/ --output final_report.html
```

---

## Key Metrics Explained

### Data Characteristics

**BCV (Biological Coefficient of Variation)**
- Low (<0.2): Cell lines, controlled conditions
- Medium (0.2-0.6): Typical experiments  
- High (>0.6): Clinical samples, complex biology
- **ML learns which pipelines handle each BCV best**

**Sequencing Depth**
- Low (<10M): May miss genes
- Medium (10-25M): Adequate
- High (>25M): Excellent

**Quality Score**
- 90-100: Excellent data quality
- 80-89: Good quality
- 70-79: Acceptable, minor issues
- <70: Poor quality, address issues first

---

## ML Confidence Levels

### High Confidence (>80%)
```
üü¢ HIGH CONFIDENCE

What it means:
‚úì Your data is very similar to many past successful analyses
‚úì ML model has strong pattern matches
‚úì Historical success rate is high (>85%)

What you should do:
‚Üí Trust this recommendation confidently
‚Üí Run pipeline, then use ATO for thresholds (NEW!)
```

### Medium Confidence (60-80%)
```
üü° MEDIUM CONFIDENCE

What it means:
‚ö†Ô∏è  Your data is somewhat similar to past analyses
‚ö†Ô∏è  Historical success rate is good (70-85%)

What you should do:
‚Üí Consider the top 2-3 recommendations
‚Üí Use ATO with 'balanced' goal for safe thresholds (NEW!)
```

### Low Confidence (<60%)
```
üî¥ LOW CONFIDENCE

What it means:
‚ùå Your data is unusual or novel
‚ùå ML model hasn't seen many similar examples

What you should do:
‚Üí Run benchmark on multiple pipelines
‚Üí Use ATO with 'discovery' goal to cast wider net (NEW!)
```

---

## üéØ ATO Goals for Different Scenarios (NEW)

After running your pipeline, choose the right ATO goal:

### Discovery Goal
```python
result = optimize_thresholds(de_results, goal='discovery')
```
**Use when:**
- Exploratory analysis
- Want more candidate genes
- Will validate later
- Pilot study

**Effect:** More permissive thresholds

### Balanced Goal (Recommended)
```python
result = optimize_thresholds(de_results, goal='balanced')
```
**Use when:**
- Standard publication
- Most typical analyses
- Good balance needed

**Effect:** Standard FDR control

### Validation Goal
```python
result = optimize_thresholds(de_results, goal='validation')
```
**Use when:**
- Clinical applications
- Confirmation study
- Need high confidence

**Effect:** Stringent thresholds

---

## Advanced Usage

### Prioritize Different Factors

```bash
# Prioritize accuracy over speed
raptor profile \
  --counts data.csv \
  --use-ml \
  --weight-accuracy 0.7 \
  --weight-speed 0.1

# Prioritize speed (quick results)
raptor profile \
  --counts data.csv \
  --use-ml \
  --weight-speed 0.6 \
  --weight-accuracy 0.2
```

### Resource Constraints

```bash
# Limited resources
raptor profile \
  --counts data.csv \
  --use-ml \
  --max-memory 16G \
  --max-runtime 2h
```

### Exclude Pipelines

```bash
# Don't recommend certain pipelines
raptor profile \
  --counts data.csv \
  --use-ml \
  --exclude-pipelines 7,8
```

---

## Quality Assessment Integration

```bash
# Run comprehensive quality check
raptor profile \
  --counts data.csv \
  --metadata metadata.csv \
  --quality-check
```

**QC Output:**
```
Quality Assessment Summary
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Overall Score: 87/100 üü¢ GOOD

Detailed Metrics:
‚îú‚îÄ Library Sizes:     92/100 ‚úÖ (CV = 12%)
‚îú‚îÄ Gene Detection:    90/100 ‚úÖ (18,234 genes)
‚îú‚îÄ Zero Inflation:    88/100 ‚úÖ (42%)
‚îú‚îÄ Batch Effects:     75/100 ‚ö†Ô∏è  (Minor detected)
‚îú‚îÄ Outliers:          80/100 ‚ö†Ô∏è  (1 flagged: Sample_7)
‚îî‚îÄ Contamination:    100/100 ‚úÖ (None detected)

Issues Found:
‚ö†Ô∏è  Minor batch effect detected
   ‚Üí Recommendation: Include batch in model

‚úÖ Data quality is good for analysis
   ‚Üí Use ATO with 'balanced' goal after pipeline runs
```

---

## Python API

### Complete Workflow

```python
from raptor import RNAseqDataProfiler, MLPipelineRecommender
from raptor.threshold_optimizer import optimize_thresholds
import pandas as pd

# 1. Load data
counts = pd.read_csv('counts.csv', index_col=0)
metadata = pd.read_csv('metadata.csv')

# 2. Profile with ML
profiler = RNAseqDataProfiler(counts, metadata, use_ml=True)
profile = profiler.profile(quality_check=True)

# 3. Get ML recommendations
ml_recommender = MLPipelineRecommender()
recommendations = ml_recommender.recommend(profile, n=3, explain=True)

# 4. View top recommendation
top = recommendations[0]
print(f"Recommended: {top['pipeline_name']}")
print(f"Confidence: {top['confidence']}")

# 5. [Run the recommended pipeline]
# raptor run --pipeline 3 ...

# 6. Load DE results and optimize thresholds (NEW!)
de_results = pd.read_csv('results/edger_results.csv')
threshold_result = optimize_thresholds(
    de_results,
    logfc_col='logFC',
    pvalue_col='PValue',
    goal='balanced'
)

print(f"\nüéØ Optimized Thresholds:")
print(f"   |logFC| > {threshold_result.logfc_threshold:.3f}")
print(f"   Significant: {threshold_result.n_significant} genes")

# 7. Get methods text for publication
print(f"\nüìù Methods:\n{threshold_result.methods_text}")

# 8. Save optimized results
threshold_result.results_df.to_csv('final_results.csv')
```

### Get Similar Projects

```python
# Find similar past analyses
similar = ml_recommender.get_similar_projects(profile, n=10)

for project in similar:
    print(f"Project {project['id']}")
    print(f"  Similarity: {project['similarity']:.3f}")
    print(f"  Pipeline used: {project['pipeline']}")
    print(f"  Success: {project['success']}")
```

### Explain Recommendation

```python
# Get feature importance
explanation = ml_recommender.explain_recommendation(profile, pipeline_id=3)

print("Feature Importance:")
for feature, importance in explanation['features']:
    print(f"  {feature}: {importance:.3f}")
```

---

## Examples

### Standard DE Study (12 samples)

**Step 1: Profile**
```bash
raptor profile --counts data.csv --use-ml
```

**Output:**
```
‚úÖ Pipeline 3 (Salmon-edgeR)
   ML Score: 0.92 | Confidence: HIGH (89%)
   
   Why: Perfect for your data
   - Medium BCV (0.42) ‚Üí Optimal range
   - 12 samples ‚Üí Ideal for this pipeline
   
   üéØ Next: Run pipeline, then optimize thresholds with ATO
```

**Step 2: Run & Optimize**
```python
# After running pipeline
result = optimize_thresholds(de_results, goal='balanced')
print(f"Use: |logFC| > {result.logfc_threshold:.2f}")
# Output: Use: |logFC| > 0.73
```

---

### Large Cohort (100 samples)

**Profile:**
```bash
raptor profile --counts large_data.csv --use-ml
```

**Output:**
```
‚úÖ Pipeline 4 (Kallisto-DESeq2)
   ML Score: 0.90 | Confidence: HIGH (88%)
   
   Why: Optimized for large datasets
   - 100 samples ‚Üí Kallisto scales well
   - Fast runtime critical for this size
   
   üéØ Next: Use ATO with 'discovery' goal for exploratory analysis
```

**Optimize:**
```python
result = optimize_thresholds(de_results, goal='discovery')
# More permissive for large exploratory study
```

---

### Small Pilot (4 samples)

**Profile:**
```bash
raptor profile --counts pilot.csv --use-ml --weight-accuracy 0.8
```

**Output:**
```
‚úÖ Pipeline 1 (STAR-RSEM-DESeq2)
   ML Score: 0.88 | Confidence: HIGH (85%)
   
   Why: Best for small samples
   - Only 4 samples ‚Üí Need robust method
   - DESeq2 handles small n well
   
   üéØ Next: Use ATO with 'validation' goal for stringent thresholds
```

**Optimize:**
```python
result = optimize_thresholds(de_results, goal='validation')
# Stringent thresholds for small, important study
```

---

## Troubleshooting

### Problem: All recommendations have low confidence

**Solutions:**
1. Check if data characteristics are extreme
2. Run quality check to identify issues
3. Consider benchmarking instead of single pipeline
4. **Use ATO with 'discovery' goal to be more permissive**

### Problem: ML disagrees with rule-based

**What to do:**
- If ML confidence is HIGH ‚Üí Trust ML
- If ML confidence is MEDIUM ‚Üí Consider both
- If ML confidence is LOW ‚Üí Trust rules
- **Always use ATO afterward for optimal thresholds**

### Problem: Quality score is low

```bash
# Get detailed quality report
raptor profile --counts data.csv --quality-check --verbose

# Address issues, then re-profile
raptor profile --counts cleaned_data.csv --use-ml

# After pipeline, ATO will still optimize for your data quality
```

---

## Best Practices

### Do's:
‚úÖ Always use `--use-ml` for recommendations  
‚úÖ Check ML confidence scores  
‚úÖ Review quality assessment  
‚úÖ **Use ATO after running pipeline (NEW!)**  
‚úÖ **Include ATO methods text in publications (NEW!)**  
‚úÖ Consider top 2-3 recommendations  

### Don'ts:
‚ùå Blindly follow low-confidence recommendations  
‚ùå Ignore quality warnings  
‚ùå **Use arbitrary thresholds - use ATO instead! (NEW!)**  
‚ùå Skip quality checks for critical analyses  
‚ùå Forget that ML is a tool, not a dictator  

---

## See Also

- [THRESHOLD_OPTIMIZER.md](THRESHOLD_OPTIMIZER.md) - üéØ Complete ATO guide (NEW!)
- [UNDERSTANDING_ML.md](UNDERSTANDING_ML.md) - ML concepts explained
- [QUALITY_ASSESSMENT.md](QUALITY_ASSESSMENT.md) - Data QC guide
- [PIPELINES.md](PIPELINES.md) - Detailed pipeline information
- [DASHBOARD.md](DASHBOARD.md) - Interactive visualization + ATO page

---

**RAPTOR v2.1.1 Profile & Recommend**  
**Author**: Ayeh Bolouki  
**License**: MIT

*"Profile smartly, recommend confidently, threshold optimally!"* üéØü¶ñ
